---
title: "R Notebook"
output: html_notebook
---

## 5.1 線性迴歸與分類 ####
## 5.1.1 多元線性迴歸 ------------------------------------------------------------------------

```{r}
library(AppliedPredictiveModeling)
# 載入溶解度資料的數個資料物件
data(solubility)
# 資料物件名都是以solT開頭的名稱
ls(pattern = "^solT")
```
## ------------------------------------------------------------------------
# 計算樣本總數

```{r}
nrow(solTrainXtrans) + nrow(solTestXtrans)

```
# 預測變數個數

```{r}
ncol(solTrainXtrans)
```
## ------------------------------------------------------------------------
# 合併屬性矩陣X與類別標籤向量y，產生統計人群慣用的資料表


```{r}
trainingData <- solTrainXtrans
trainingData
```


```{r}
trainingData$Solubility <- solTrainY
head(trainingData$Solubility)
```

# R語言線性迴歸建模的主要函數lm()
```{r}
lmFitAllPredictors <- lm(Solubility ~ ., data = trainingData)
```

# 配適好的模型其類別與建模函數同名
```{r}
class(lmFitAllPredictors)
```

# summary.lm()產生R語言線性迴歸摘要報表lmAllRpt
```{r}
lmAllRpt <- summary(lmFitAllPredictors)
```

# 229個迴歸係數報表很長，僅挑出其t檢定顯著水準低於5%者
```{r}
sigVars <- lmAllRpt$coefficients[,"Pr(>|t|)"] < .05
```


# 54個係數的顯著水準低於5%(至少一星*)
```{r}
sum(sigVars)
```


# 邏輯值索引只檢視54個顯著的迴歸係數
# 更新原229項龐大的迴歸係數報表
```{r}
lmAllRpt$coefficients <- lmAllRpt$coefficients[sigVars, ]
lmAllRpt

```
## ------------------------------------------------------------------------
# predict.lm()預測測試樣本溶解度

```{r}

lmPred1 <- predict(lmFitAllPredictors, solTestXtrans)
head(lmPred1)

```

# 測試集實際值與預測值組成資料框

```{r}

lmValues1 <- data.frame(obs = solTestY, pred = lmPred1)
lmValues1
```
```{r}
# R語言{caret}套件績效評量計算函數
defaultSummary(lmValues1)
```
```{r}
# 後向式逐步迴歸step()需傳入完整模型，再逐次剔除不重要變數
# 建模時間長，system.time()衡量程式碼執行時間(1.9節)
# 逐步迴歸建模過程AIC或BIC值愈小，模型配適的愈好(3.2.1節)
system.time(reducedSolMdl <- step(lmFitAllPredictors,
direction='backward'))


```

# 儲存執行耗時的配適結果
```{r}
save(reducedSolMdl, file = "reducedSolMdl.RData")
```

```{r}
# 因模型建立耗時，載入預先跑好的模型物件
load("reducedSolMdl.RData")
```
# 原始報表過長，請讀者自行執行程式碼

```{r}
summary(reducedSolMdl)
```

## ----eval=FALSE----------------------------------------------------------

```{r}
# 129個模型項代表原228個變數，後向式逐步迴歸挑選了128個入模
str(coef(reducedSolMdl))

```

# 後向式逐步迴歸摘要報表lmBackRpt
```{r}
lmBackRpt <- summary(reducedSolMdl)
lmBackRpt
```


# 因迴歸報表很長，挑出t檢定顯著水準低於5%的模型項
```{r}
sigVars <- lmBackRpt$coefficients[,"Pr(>|t|)"] < .05
```

# 96個入模變數的係數顯著水準低於5%(至少一星*)
```{r}
sum(sigVars)
```
# 更新原129項的後向式逐步迴歸係數報表

```{r}
lmBackRpt$coefficients <- lmBackRpt$coefficients[sigVars, ]
```
# 先建立只有截距項的最簡模型

```{r}
minSolMdl <- lm(Solubility ~ 1, data = trainingData)

```
# 前向式逐步迴歸step()需傳入最簡模型，再逐次增加變數入模
# as.formula()設定scope引數的最複雜模型公式

```{r}
system.time(fwdSolMdl <-step(minSolMdl, direction='forward'
, scope = as.formula(paste("~", paste(names(solTrainXtrans)
, collapse = "+"))), trace=0))
```


# 儲存執行耗時的配適結果

```{r}
save(fwdSolMdl, file = "fwdSolMdl.RData")

```

# 因模型建立耗時，載入預先跑好的模型物件
```{r}
load("./_data/fwdSolMdl.RData")
```

# 原始報表過長，請讀者自行執行程式碼

```{r}
summary(fwdSolMdl)
```

```{r}
## ------------------------------------------------------------------------
# 前向式逐步迴歸摘要報表lmFwdRpt
lmFwdRpt <- summary(fwdSolMdl)
# 因迴歸報表很長，以t檢定顯著水準低於5%的標準縮減報表
sigVars <- lmFwdRpt$coefficients[,"Pr(>|t|)"] < .05
# 76個入模變數的係數顯著水準低於5%(至少一星*)
sum(sigVars)
# 更新原115項的前向式逐步迴歸係數報表
lmFwdRpt$coefficients <- lmFwdRpt$coefficients[sigVars, ]

## ----eval=FALSE----------------------------------------------------------
# 更新後的報表仍然相當長
lmFwdRpt

## ----eval=FALSE----------------------------------------------------------
# 前向式與後向式(較大)逐步迴歸模型ANOVA比較(結果顯著)
# ANOVA報表中1: fwdSolMdl, 2: reducedSolMdl
anova(fwdSolMdl, reducedSolMdl)
# 請自行檢視實際呼叫之檢定函數的說明文件
# ?anova.lm()

## ----echo=FALSE----------------------------------------------------------
data.frame(anova(fwdSolMdl, reducedSolMdl), check.names = FALSE)

## ----eval=FALSE----------------------------------------------------------
# 後向式與完整(較大)逐步迴歸模型ANOVA比較(結果不顯著)
# ANOVA報表中1: reducedSolMdl, 2: lmFitAllPredictors
anova(reducedSolMdl, lmFitAllPredictors)

## ----echo=FALSE----------------------------------------------------------
data.frame(anova(reducedSolMdl, lmFitAllPredictors), check.names = FALSE)

```


## 5.1.2 偏最小平方法迴歸 ####
## ----warning=FALSE, message=FALSE----------------------------------------
# 載入R語言偏最小平方法估計套件
```{r}
library(pls)
# 模型公式語法擬合模型
plsFit <- plsr(Solubility ~ ., data = trainingData)
plsFit
```


```{r}
# 凡事總有例外，mvr類別物件
class(plsFit)
```


```{r}
# 擬合結果摘要報表
summary(plsFit)
```
## ----fig.align="center", fig.cap = "\\label{fig:pls_screeplot}RMSEP對成份個數的陡坡圖"----
# 繪製PLS決定主成份個數的陡坡圖
# plottype引數決定繪製不同主成份下的核驗統計值(預設為RMSEP)
```{r}
plot(plsFit, plottype = "validation")
```


```{r}

## ----fig.align="center", fig.cap = "\\label{fig:pls_screeplot}PLS在9個主成份下預測值對實際值的散佈圖"----
# 繪製9個PLS主成份下，訓練集之預測值對實際值的散佈圖
plot(plsFit, ncomp = 9)
# 9個PLS主成份下，訓練集之預測值對實際值的相關係數
cor(plot(plsFit, ncomp = 9)[,"measured"], plot(plsFit,
ncomp = 9)[,"predicted"])
# 9個PLS主成份下的模型，預測測試集樣本solTestXtrans溶解度
pre <- predict(plsFit, solTestXtrans, ncomp = 9)
# 測試集預測值與實際值的相關係數
cor(pre[, 1, 1], solTestY) # 請自行檢視str(pre)

```



## 5.1.3 脊迴歸、LASSO迴歸與彈性網罩懲罰模型 ------------------------------------------------------------------------
# 設定待調懲罰係數值



```{r}
library(caret)
ridgeGrid <- expand.grid(lambda = seq(0, .1, length = 15))
# 十摺交叉驗證參數調校訓練與測試
ctrl <- trainControl(method = "cv", number = 10)
set.seed(100)
```

```{r}
library(foreach)
library(doParallel)
# How many cores does your CPU have
n_cores <- detectCores()
n_cores


```
```{r}

cluster <- makeCluster(n_cores - 1)
registerDoParallel(cluster)

```

```{r}
# How many times will the loop run
n_iterations <- 1000
# To save the results
results <- list()
# Use foreach and %dopar% to run the loop in parallel
results <- foreach(i = 1:n_iterations) %dopar% {
  # Store the results
  results[i] <- i^2
}
# Don't fotget to stop the cluster
stopCluster(cl = cluster)
```


Windows作業系統多核運算
```{r}
install.packages("doParallel")
install.packages("snow")
```


```{r}
# 加載所需的包
library(caret)
library(doParallel)
library(snow)
cl <- makeCluster(n_cores-1)
registerDoParallel(cl)
```

請注意本節train()函數並未使用模型公式語法，與第三章不同

```{r}
system.time(ridgeTune <- train(
x = solTrainXtrans, # 校驗集屬性矩陣
y = solTrainY, # 類別標籤向量
method = "ridge", # 訓練方法
tuneGrid = ridgeGrid, # 待調參數網格
trControl = ctrl, # 訓練測試機制
preProc=c("center","scale"))) # 前處理方式
```

儲存耗時參數校驗結果
```{r}
save(ridgeTune, file = "ridgeTune.RData")
```


```{r}
## ----fig.align="center", fig.cap = "\\label{fig:ridge_tuningplot}脊迴歸不同懲罰係數下的模型績效概況圖"----
# 因模型建立與調校耗時，載入預先跑好的模型物件
load("ridgeTune.RData")
ridgeTune
```


```{r}
# 不同懲罰係數下，十摺交叉驗證平均RMSE折線圖
plot(ridgeTune, xlab = 'Penalty')

## ------------------------------------------------------------------------
# 兩個待調參數形成的3*20網格
enetGrid <- expand.grid(lambda = c(0, 0.01, .1), fraction
= seq(.05, 1, length = 20))
set.seed(100)
```

# 引數訓練方法method改為enet
```{r}
system.time(enetTune <- train(x = solTrainXtrans,
                              y = solTrainY,
                              method = "enet",
                              tuneGrid = enetGrid,
                              trControl = ctrl,
                              preProc = c("center", "scale")))
```


```{r}
# 儲存耗時參數校驗結果
save(enetTune, file = "enetTune.RData")
```



## ----fig.align="center", fig.cap = "\\label{fig:enets_tuningplot}彈性網罩模型不同參數組合下的模型績效概況圖"----

# 因模型建立與調校耗時，載入預先跑好的模型物件
```{r}
load("enetTune.RData")
```


```{r}
enetTune
```

# 參數調校模型物件的類別為train
```{r}
class(enetTune)
```

# 不同參數組合下，交叉驗證績效概況
```{r}
plot(enetTune)

```


 # need to try below codes


```{r}
## 5.1.4 線性判別分析 ------------------------------------------------------------------------
# 第1類樣本母體平均值向量
(mu1 <- c(1,1))
# 第2類樣本母體平均值向量
(mu2 <- c(3.5,2))
# 兩類樣本共同的母體共變異數矩陣
(sig <- matrix(c(1,0.85,0.85,2), ncol = 2))

## ------------------------------------------------------------------------
# 載入R語言多元常態分佈隨機抽樣函數
library(mvtnorm)
n1 <- 1000*0.9
n2 <- 1000*0.1
# 定義0-1類別標籤向量，0與1各重複n1與n2次
group <- c(rep(0,n1), rep(1,n2))
set.seed(130)
```


```{r}
# 模擬抽出第1類的二維訓練樣本
X1train <- rmvnorm(n1, mu1, sig)
# 模擬抽出第2類的二維訓練樣本
X2train <- rmvnorm(n2, mu2, sig)
# 合併兩類訓練樣本為屬性矩陣
Xtrain <- rbind(X1train, X2train)
# 屬性矩陣與類別標籤組織為訓練資料集
dtrain <- data.frame(X = Xtrain, group = group)
dtrain[898:903,]
set.seed(131)
# 測試資料集同前模擬與處理
X1test <- rmvnorm(n1,mu1,sig)
X2test <- rmvnorm(n2,mu2,sig)
Xtest <- rbind(X1test,X2test)
dtest <- data.frame(X = Xtest,group = group)
dtest[898:903,]
```


```{r}
## ----fig.align="center", fig.cap = "\\label{fig:rmvnorm_scatterplot}訓練與測試兩類樣本的散佈圖"----
# 觀察兩類樣本在各子集散佈狀況是否相似
op <- par(mfrow = c(1,2))
plot(dtrain$X.1, dtrain$X.2, pch = dtrain$group + 1, main =
"Training data", xlab = expression(x[1]),
ylab = expression(x[2]))
legend("bottomright", c("Cl1","Cl2"), pch = 1:2)
plot(dtest$X.1, dtest$X.2, pch = dtest$group + 1, main =
"Test data",xlab = expression(x[1]),ylab = expression(x[2]))
legend("bottomright", c("Cl1","Cl2"), pch = 1:2)
par(op)
```


```{r}
## ------------------------------------------------------------------------
# 擬取用套件{MASS}下的lda()
library(MASS)
# 訓練模型
resLDA <- lda(group~., data = dtrain)
# 預測測試資料的類別隸屬度
predLDA <- predict(resLDA, newdata = dtest)$class
# 混淆矩陣
table(dtest$group, predLDA)
# 正確率計算
mean(dtest$group == predLDA)
```
## 5.1.5 羅吉斯迴歸分類與廣義線性模型 ####
## ----warning=FALSE, message=FALSE----------------------------------------
# 誤差分佈為二項式時，連結函數預設為logit

```{r}

resLR <- glm(group~., data = dtrain, family = binomial)
# 羅吉斯迴歸的預測值是關心事件發生機率的logit值
predLogit <- predict(resLR, newdata = dtest)
head(predLogit)
```

# 套件{boot}的inv.logit()函數，將預測值逆轉換回機率值
```{r}
library(boot)
predProb <- inv.logit(predLogit)
head(predProb)
# 以0.5為門檻值，機率再轉成類別標籤預測值
predLabel <- predProb > 0.5
head(predLabel)
# 混淆矩陣
table(dtest$group, predLabel)
# 正確率計算
mean(dtest$group == predLabel)

```

## 5.2.4 分類與迴歸樹 ####
## ----warning=FALSE, message=FALSE----------------------------------------
# R語言Recursive PARTitioning遞迴分割建樹套件
```{r}
library(rpart)
head(iris)
# 以鳶尾花花瓣花萼長寬預測花種
iristree <- rpart(Species ~ ., data = iris)

## ----eval=FALSE----------------------------------------------------------
# 分類樹模型報表，與樹狀圖的說明相同
iristree

## ----fig.align="center", fig.cap = "\\label{fig:iris_ct}鳶尾花資料集分類樹"----
# 載入R語言樹狀模型繪圖套件{rpart.plot}，輕鬆視覺化分類樹模型
library(rpart.plot)
rpart.plot(iristree, digits = 3)

## ------------------------------------------------------------------------
# 匯入報稅稽核資料集，納稅義務人人口統計變數與稽核結果
audit <- read.csv('./_data/audit.csv')
head(audit)
# 目標變數稽核後是否有修正的(0: 無，1: 有)次數分佈
table(audit$TARGET_Adjusted)

## ------------------------------------------------------------------------
# 載入模型物件ct.audit
load("./_data/ct.audit.RData")

## ----eval=FALSE----------------------------------------------------------
# 分類樹模型報表(Too wide to show here. 參見圖5.17)
ct.audit

## ----message=FALSE, warning=FALSE, fig.align="center", fig.cap = "\\label{fig:before_pruned_ct}修剪前的分類樹"----
# 修剪前分類樹模型視覺化
library(rpart.plot)
rpart.plot(ct.audit, roundint = FALSE)

## ----CP------------------------------------------------------------------
# 取出複雜度參數表(表5.5 cptable)
knitr::kable(
  ct.audit$cptable, caption = '分類樹複雜度參數表',
  booktabs = TRUE
)

## ------------------------------------------------------------------------
# 從cptable定位交叉驗證錯誤率的最小值
(opt <- which.min(ct.audit$cptable[,"xerror"]))
# 從cptable定位相對錯誤率小於交叉驗證的最小錯誤率
# ，加上其對應的一倍標準誤
(oneSe <- which(ct.audit$cptable[, "rel error"] <
ct.audit$cptable[opt, "xerror"] +
ct.audit$cptable[opt, "xstd"])[1])
# 取得one-SE準則下的最佳CP值
(cpOneSe <- ct.audit$cptable[oneSe, "CP"])
# cpOneSe輸入prune()函數，完成one-SE事後修剪
ct.audit_pruneOneSe <- prune(ct.audit, cp = cpOneSe)

## ----fig.align="center", fig.cap = "\\label{fig:after_pruned_ct}修剪後的分類樹"----
# 繪製修剪後的分類樹圖形
rpart.plot(ct.audit_pruneOneSe, roundint = FALSE)



```

